\chapter{学术界评论}


\section{武大教授怒批：如何迅速成为高校著名教授}
%来源： 微信公众号
作者：孟勤国，武汉大学法学院教授，广西大学法学院名誉院长


经常和学生笑谈硕士、博士的前程，我的态度毫不含糊。

第一选择当然是从政。当了官就有话语权，可以坐学术会议的主席台，谈几点谦称为个人意见的指示，台下昔日的老师和同窗隆重地仰望着你，为你是专家型官员而自豪。

学问跟着级别长，只要官长到厅级以上，你就有学问，而且你的手下通常会感觉你的学问盖过了当年的各学科前辈。

第二选挣钱。当大富豪或大商人，银子多到懒得数，拿点出来设个研究基金或课题，你昔日的老师和同窗会按时拿出你要求的成果，在成果扉页上鸣谢你这位大商人。

学术跟着银子走，只要捐出一个亿，你想要哪个学院姓你的姓，随你挑。

没能耐从政和挣钱，只能选择第三去大学教书。混20年，当个名义上与副厅平起平坐、实际上常挨教务处小科长修理的教授不成问题。但这有点窝囊，不如费点劲，争取当个看起来真有点副厅以上模样，活得滋润、活得牛气的著名教授。

人人都有成佛的慧根，当著名教授一靠努力，二靠方法。在确定了努力的方向以后，方法就是决定的因素，如果你定下著名教授的远大志向，应该熟识以下三招：



第一招：搏命著书，绝不立说。        



讲师、副教授、教授，一路晋升，全靠科研。


“没有数量就没有质量”的哲学观点在中国已经发展为“数量就是质量”，因而什么事都以总量论英雄。没有一大堆的书和文章，一般评不上教授；没有一麻袋以上的书和文章，绝对不可能是著名教授。

在尽可能短的时间内出尽可能多的书和文章，就能尽快当上教授；当上了教授，就可以招脑瓜子灵活的研究生，抓他们来编更多的书和文章，形成良性循环。

快出、多出成果，除了需要一台编辑功能强大的电脑以外，还得知道两个秘诀。


一是在没出名时舍得支付版面费、书号费。付了费没人管你句子写得通不通，文章就能变成铅字，职称文件规定：没变成铅字一律不算成果。一篇文章一千，十篇也就是一万（现在好像涨价了，不过也没关系，成功总要付出代价，舍不得孩子套不住狼嘛），如果职称上一个档次，也就是花费一年不到的新增的工资和津贴。第二年开始就净赚，这个账一定要会算。



二是坚持人云亦云原则，不想标新立异的事儿。

古人著书是为了立言，今人著书是为了谋生，时代不同了，著书不一样了。

学术圈内聪明人不少，但没有什么天才，天才都选有诺贝尔奖的专业。既然不是天才，就难以文思泉涌，弄出一麻袋的新思想、新观点、新见解。在已经被翻掘了无数遍的学术田野上，刨出独特的学术见解本就不容易，再要自圆其说，很耗费光阴。

学古人将著书和立说连在一起，你若信了，这辈子也就是一个资深讲师。再说，你说的与主流不同，著名的同行会很生气，会说你错、不懂、没水平，在评审时给一个不合格，后果很严重。

人云亦云，同时在你的书和文章中多引用写在世同行最好是著名同行的书和文章，容易形成良好的学术人脉。著名的同行一高兴，说不准给你的成果赐一篇悼词般的序，你拿着去申报什么国家千人计划、万人工程或什么江河湖海学者就很有底气。

你实在忍不住想说点儿不同，也一定要注明是浅见或抛砖引玉。


现在的年代，没人在乎你说了什么，只在乎你说了没有，只要书店到处是你主编的什么新论、什么论丛、什么大全，你不想著名都不行。

第二招：糊涂上课，煽情演讲 。     

郑板桥有一至理名言：“难得糊涂。”这四个字体现了东方哲学的最高的境界。给本科生上课，将基本概念、基本知识、基本原理讲得那么清楚，学生给打70分就算对得起你，简单意味着没有神秘感，没有神秘感哪有崇拜可言？

在课堂上，不将简单的问题复杂到学生找不到北，不足以显示学问的博大精深。你试着讲讲主观说、客观说、折中说，背诵几段教科书里很难找的文献或几段拉丁美洲一般人都看不到的论述，再穿插几段其它著名学者的花絮或八卦，看哪个学生的眼神不迷茫、不流露出崇敬？

学生听得云里雾里，就会到处显摆听过有学问的课，慢慢地你就声名远播。

将本科生当成研究生上课、将研究生当场本科生上课，这样一换位，本科生崇拜你学富五车，研究生崇拜你记忆过人——他们已经忘光了的基础知识你居然一清二楚。





如果有哪个论坛或沙龙请你当嘉宾，一定要去，不讲条件——成了著名教授后才可以与邀请方讨论出场费问题。嘉宾通常要做或长或短的演讲。每一次演讲都是积累知名度的机会，绝不能浪费在平淡无奇之中。

小小骂一下中国的现行体制或某一当局以显示你的独立和勇气，大大翻一下“风可进、雨可进、国王不可进”之类的典故或传说以显示你的博学和信仰，狠狠批一下对著名的同行大不敬的持不同“学”见者以显示你的忠诚和能力，这些能使你的演讲跌宕起伏，煽起一片赞叹之声。

能声情并茂更好，没这个能力，要敢于牵强附会、指鹿为马、断章取义。

哪怕是无中生有，只要你以一种断然的口气说“孟勤国主张农民有种地的义务”之类的，听众自然深信不疑。人气是著名教授的魂魄，一定不能散了，要想法设法以不能流芳百世就争遗臭万年的心态积聚和保持人气。





第三招：出国镀金，进京扬名。      

在著名的同行心目中，中国学术的主要矛盾是落后的中国社会生活跟不上先进的西方理论的需要，因而中国学术的主要任务是以西方理论改造落后的社会生活。

出国留学了，你就不再是落后的社会生活的一分子，就有权代表先进的西方理论，即便你赞同学术应适应实际生活需要，也不算是著名同行所斥责的受前苏联意识形态毒害的国情论，而属于著名同行追捧的本土资源论。

当然，出国留学不能像胡适或以前的教授那样在外国的大学一待七八年，想“著名”的教授很多，在国外耽误时间非常影响竞争力。出国留学的最佳时间是一年左右，前半年熟悉当地的风土人情，后半年收拾行李准备回家。

出国留学这个词的重音在“留”而不在“学”，“留”就是溜达的意思。



学习西方理论是为了劝说中国的学术圈照搬外国的理论，知其然足矣，读点外国理论什么后现代啊什么公地悲剧的中译本绰绰有余，没有必要在外国傻啃外文书。外文书读多了，万一发现外国理论有个什么短处，有损外国理论的高大形象和普适性。

记住：留学回来，不要再回原来的单位，一定要进北京城。

首选北大、清华，这两个学校王气冲天，谁沾上了至少是半个王。进不了，进北京其他学校也行，哪怕这个学校只招三本。北京是首都，也是著名教授的唯一原产地。

在北京，能经常出入国家立法机关、上央视频道、拿到国家级的重大攻关课题，随便说个什么就上中央媒体弄得地球人都知道。好像有学者说过，外地放十炮不如北京放一枪，此言极为传神。京城以外很少有“著名”的机会，就算偶尔“著名”了，也不能算是正宗，除非攀上京城某一著名同行的小圈子。




京城内外，学识标准有别，评价当然也不一样。进了北京城，你可以信口开河，对着全国的电视观众发表像孔庆东之类的惊世骇言，这属于新锐观点，而同样的事落在你进北京城之前，就属于违背常识，要挨板砖打的。

进了北京城，你有更多的机会谋一个中国什么研究会的副会长，这也算是你这行的国家领导人了，如果谋上了可不得了，走到哪里都有人勾兑你，求你在评审什么时高抬贵手。

肚子里的学问只有自己知道，身上的光环谁都能见到，光环越多、越耀眼，就越容易折服芸芸众生。




不是所有的学生都相信上述三招，有大胆的学生质疑我怎么就不用这三招。我不能故作高尚说我不用这三招是因为不屑于“著名”或不屑于这样“著名”，实在是我想“著名”的时候没有人给我指点如何“著名”——已经“著名”的绝对不会将自己“著名”的经验告诉别人，等到我琢磨出“著名”之道时我已经白发苍苍——身处人生下半场再想着“著名”实属老不正经。

其实，成功者的经验大多是靠不住的，什么上大学时拉小提琴的时间多于学习的时间之类，不过是炫耀聪明或掩饰阴暗，相信而且模仿这等成功经验等于是找电线杆上小广告上的游医治病。真正有用的常常是未成功者的人生体验，因为内含的是长期的生活观察和经验教训。

当然，谓有用，完全是在功利层面上的，如果加上人生观和价值观的因素，那就是仁者见仁、智者见智了。


\section{深度学习先驱Yann LeCun被骂退推特}
%来自： 微信朋友圈、公众号

「我请求社交网络上的所有人不要再互相攻击了，特别是对于 Timnit Gebru 的攻击，以及对于我之前一些言论的攻击。」Yann LeCun 刚刚在推特上发出了这样的呼吁。「无论是口头还是其他方式的冲突，都只能获得伤害和相反的结果。我反对一切形式的歧视。这里有一篇关于我核心价值观的文章。」

「这是我在推特上最后一篇有内容的帖子，大家再见。」




看起来 2018 年图灵奖得主、人工智能领军人物 Yann LeCun 已经下定决心想对长达两周的激烈讨论画上句号。而这场闹得沸沸扬扬的骂战，起因正是被指「严重种族歧视」的 PULSE 算法。

这一工作由杜克大学推出，其人工智能算法可以将模糊的照片秒变清晰，效果极佳。这项研究的论文已在 CVPR 2020 上发表（论文《PULSE：Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models》）。




PULSE 在 CVPR 大会期间引来了人们的关注，进而引发了 AI 社区的广泛争议。首先，这种方法所产出的图像清晰度更高，细节也更加丰富：PULSE 能够在几秒内将一张 16×16 像素的图片提升至 1024×1024 分辨率，提升高达 4096 倍。目前该算法仅针对人脸照片，算法生成的照片清晰到可以呈现出人脸上的毛孔、皱纹甚至一缕头发。

但本质上看，PULSE 并不是在消除马赛克，而是「生成」了看上去真实却并不存在的人脸。超分辨率算法一直是计算机科学的热门研究领域，以往科学家们提出的还原方法是在低分辨率图片中添加像素点。但 PULSE 使用 GAN 的思路，先利用深度学习算法生成一些高清图片，再降低它们的分辨率，并与模糊的原图对比，从中找出匹配程度最高、最接近原图的高清图像随后输出。

问题就出在这里，有网友试用了 PULSE 之后，发现美国前总统奥巴马的高糊照片经过去码处理以后，生成的是一张「白人面孔」：




有网友质疑该方法生成结果存在偏见，对此项目作者也给出了回应，表示这一偏见很可能来自于 StyleGAN 的训练数据集，可能还有其他未知因素。

「我们意识到偏见是机器学习和计算机视觉领域的重要问题，并就此问题联系了 StyleGAN 和 FFHQ 数据集的创建者。我们希望这能够促进不具备此类偏见行为的方法的诞生。」

但这件事还没完，鉴于美国目前 BLM 的舆论环境，人们很快就开始深入讨论机器学习研究结果缺乏多样性的问题。在这其中，种族偏见和性别偏见的问题一直存在，迄今为止却没人给出一个好的解决办法。 

也就在这个时候，Yann LeCun 发布了一条推特，来解释为什么 PULSE 会出现这样的偏见。




「机器学习系统的偏差是因为数据的偏差。这一人脸上采样系统其结果倾向于白人是因为神经网络是在 FlickFaceHQ 上预训练的，其中的大部分图片基本是白人照片，」Yann LeCun 说道。「如果这一系统用塞内加尔的数据集训练，那肯定所有结果看起来都像非洲人。」

Yann LeCun 的说法本身没有错，但可能是因为过于直白了，一下子让大量 AI 从业者和研究人员炸了锅。LeCun 希望将人们的注意力引向数据集的偏差，但推特网友不买帐，并指责他「用这种陈旧的理由来掩盖问题本质」。

之后，Yann LeCun 又在多条推文来解释自己关于偏见的立场，但仿佛已经没有用了。




「与学术论文相比，这种偏见在已经部署的产品中产生的后果会更加可怕。」这句话的含义被解读为「不必为此特例而过分担心」，引发了诸多同行的质疑。

斯坦福 AI Lab 成员、Google AI 科学家 Timnit Gebru（她是一名非洲裔美国人），对 LeCun 的言论表示「失望」。

Yann LeCun 甚至在 Timnit Gebru 的推特评论区连写 17 条回复：




当然，需要讨论的也不只是机器学习中的偏见问题：




「同样需要避免的是在对话中产生恶意，它只会激起情绪，伤害到所有人，掩盖实际问题，推迟解决方案的出现。」




从事数据科学领域超过十年的 Luca Massaron 认为，尽管从技术角度来看 Yann LeCun 是完全正确的，但看看这种观点被抛出之后公众的反应，你就会知道谈论它是多么的敏感。

「人们总是害怕自己会被不公平的规则控制，进而无条件地，有时甚至毫无理由地惧怕 AI 剥夺人们的自由，而不仅仅是工作，」Luca Massaron 说道。「我个人并不担心 Face Depixelizer 这类研究，我所害怕的是在应用之后，我们无法识别和挑战偏见。」




如今，越来越多的机器学习自动化技术正在进入我们的生活，立法者在这里扮演的角色非常重要。在欧盟国家，为了确保数据使用的透明度和责任，GDPR 条例要求互联网公司保证算法的可解释性，以及用户对于自身数据的控制力。

如果我们希望 AI 能够朝着正确的方向发展，我们需要追求的或许不是无偏见，而是透明度。Luca 认为，如果算法是有偏见的，我们可以挑战它的推断结果并解决问题。但如果算法的推理机制不可知，或许其中还隐藏着更大的问题。




不可否认的是，人类社会存在着各种偏见，但因此而认为机器倾向于更「流行」的答案是理所应当的，或许不是一个正确的观点。



人们对于 PULSE 的讨论，以及 LeCun 的攻击，有很多已脱离了 LeCun 的本意。

作为这场争议的起因，杜克大学的研究者们已在 PULSE 网站中表示将会修正有关偏见的问题。目前论文中已经增加了一个新的部分，并附加了可以解决偏差的模型卡。

为了达成没有偏见的目标，我们必须让整个人工智能社区行动起来。但在有关技术的讨论之中让技术大牛心灰意冷，是大多数人都不想看到的结果。Yann LeCun 此前一直以直言不讳著称，他在社交网络上经常会对热门的深度学习研究发表评论，也可以直面其他人工智能著名研究者的批评。

机器学习模型中的偏见可能会使得推理的专业性受到侵害，导致大量业务遭受影响却不为人所知。我们还没有解决这个问题一劳永逸的方法。

参考内容：
https://analyticsindiamag.com/yann-lecun-machine-learning-bias-debate/


\section{近亿级数据集下线，MIT道歉，ImageNet 亦或遭殃}

麻省理工学院（MIT）已永久删除包含8000万张图像的Tiny Images数据集。
此举是论文《Large image datasets: A pyrrhic win for computer vision?》中的发现导致的结果。论文作者在数据集中发现了许多有危害类别，包括种族歧视和性别歧视。这是依赖WordNet名词来确定可能的类别而没有检查图像标签带来的结果。他们还确定ImageNet中也有类似的问题，包括非自愿的色情材料等。
在The Register向MIT发出警示之后，该数据集已于本周删除。MIT还敦促研究人员和开发人员停止使用该数据集，并删除任何副本。CSAIL的电气工程和计算机科学教授Antonio Torralba表示：“实验室根本不知道这些令人反感的图像和标签存在于数据集中。”他告诉The Register：“很明显，我们应该手动筛选它们。为此，我们深表歉意。”
由于MIT在采集数据集时使用不当的方法，这些系统可能将女性标记为“ji女”或“biao子”，而对黑人和亚裔的描述则带有贬义。该数据库还包含标有“cunt”的女性生殖器特写图片，此外还包括带有“nigger”（黑鬼）标记的黑人和猴子的图片，穿着比基尼或抱着孩子的妇女，被贴上“ji女”的标签，将日常图像与诽谤、令人反感的语言联系起来，并把偏见引入AI模型。      
       该图展示了MIT数据集中标有问题单词的图片数量。

Tiny Images数据集可视化下线之前的屏幕快照。它展示了标签“ji女”的数据集示例，出于法律原因，已将其像素化。图片包括母亲抱着婴儿的照片、圣诞老人的爆头照片、色情女演员和穿着比基尼的女人的照片。
如今，Tiny Images数据集与更知名的ImageNet数据集都成为了评估计算机视觉算法的基准。但是，与ImageNet不同，到目前为止，还没有人检查过Tiny Images中有问题的内容。
ImageNet也存在相同的问题，因为它也使用WordNet进行了标记。名为ImageNet Roulette的实验让人们将照片提交到ImageNet训练的神经网络，一些人上传了自拍照，但是当软件使用种族主义和冒犯性标签描述他们时，他们感到震惊。
在这些庞大的数据集中，有问题的图像和标签所占的比例很小，很容易将它们当作异常现象而忽视掉。这部分数据集在AI训练过程中通常不能得到均衡的分配。这就是面部识别算法难以识别女性和肤色较深的人的原因。底特律的一个黑人在今年早些时候被面部识别软件误认为是可疑小偷后，被警察误捕。近期颇有争议的图像翻译算法PULSE则将奥巴马的模糊照片变成了白种人。
 
1


祸起WordNet 
Torralba教授介绍了Tiny Images数据集的构建方式：获得大量单词（包括贬义词），然后编写代码以使用这些单词在网络上搜索图像并将其结合在一起。
Torralba教授说：“数据集包含直接从WordNet复制的53,464个不同名词”然后，这些数据被用来从互联网搜索引擎自动下载相应名词的图像，最后使用当时可用的过滤器来收集8000万张图片。”
WordNet于1980年代中期在普林斯顿认知科学实验室建立，由George Armitage Miller创立，他是认知心理学的创始人之一。“ Miller着迷于单词之间的关系，Prabhu说：“数据库本质上反映了单词如何相互关联。”
例如，“猫”和“狗”比“猫”和“伞”更紧密相关。不幸的是，WordNet中的某些名词是种族歧视的和侮辱性的。几十年后的今天，这些术语困扰着现代机器学习。
“在构建庞大的数据集时，需要某种结构，” Birhane说：“这就是WordNet有效的原因。它为计算机视觉研究人员提供了一种对图像进行分类和标记的方法。当可以使用WordNet时，为什么要自己手动做呢？”
 
2


Tiny Images和ImageNet的批判研究
回到这件事的起因上，该论文的两位作者是来自硅谷一家隐私初创公司UnifyID的首席科学家Vinay Prabhu和爱尔兰都柏林大学的博士学位候选人Abeba Birhane，他们在研究了MIT数据库之后发现了成千上万张带有针对黑人和亚洲人的种族主义诽谤和用于描述女性的贬义词标签的图像。之后他们以ImageNet-ILSVRC-2012数据集为例做了一些研究并发表了本篇论文。
作者调查了由于不严格且考虑不周的数据集管理做法而导致的整个社会以及个人所面临的危害和威胁的情况，并且提出可能的纠正方法，并批评这些方法的利弊。作者适当开源了在此努力中生成的所有代码和普查元数据集，以使计算机视觉社区得以建立。通过揭露威胁的严重性，作者希望激发大型数据集管理流程的强制性机构审查委员会（IRB）的组成。
作者认为在大数据时代，个人知情同意、隐私权或代理权的基本原则已逐渐被侵蚀。机构、学术界和工业界，在未经同意的情况下收集了数以百万计的人的图像。如表1所示，在同行评议的文献中发现了数以千万计的人物形象。这些图片是在未经个人同意或知情的情况下获得的，也未经IRB批准收集。

作者对ImageNet数据集进行了批判：
ImageNet数据集的出现被广泛认为是深度学习革命中的一个关键时刻，它改变了计算机视觉和人工智能。从图像的可疑方式的来源，到图像中人物的标记，再到使用这些图像训练人工智能模型的下游效果，ImageNet和大规模视觉数据集（LSVD）总体上构成了计算机视觉的一个代价高昂的胜利。这场胜利是以伤害少数群体为代价的，并进一步助长了对个人和集体的隐私和知情权的逐渐侵蚀。当更广泛的计算机视觉社区缺乏对ImageNet数据集的审查，这只会鼓励学术和商业机构在没有审查的情况下建立更大的数据集。
随之作者又进行了一些反思：
大型图像数据集，如果没有仔细考虑社会影响，就会对个人的福利和福利构成威胁。允许人脸搜索的反向图像搜索引擎在过去的一年里取得了显著而令人担忧的效率。只需支付少量费用，任何人都可以使用他们的门户或API来运行一个自动化程序以发现ImageNet数据集中人类的“真实”身份。例如，在性工作受到社会谴责或法律定罪的社会中，通过图像搜索重新识别性工作者，对受害者个人来说确实是一种危险。
说到这里我们额外提一句，以上事情在中国也切切实实的正在发生着，国内某家搜索引擎巨头的老板曾在前年中国发展高层论坛现场就人们关心的数据和隐私问题谈到：“中国人更加开放，对隐私问题没有那么敏感，如果他们可以用隐私交换便捷性，很多情况下他们是愿意的。”
哦，怪不得他之后在自家公司的大会上被人泼了"宏颜祸水"，另外这家公司出品的“百毒”识图相信大家也都用过。
最后作者给了一些解决方案建议：
1、合成真实和数据集蒸馏
这里的基本思想是在模型训练期间使用（或增强）合成图像来代替真实图像。方法包括使用手绘草图图像（imagenet sketch），使用GAN生成的图像和数据集蒸馏等技术，其中一个数据集或一个数据集的子集被提炼成几个具有代表性的合成样本。这是一个新兴的领域，在跨视觉域的无监督域适应和通用数字分类方面有一些有希望的结果。
2、对数据集强化伦理过滤
3、定量数据集审计：以ImageNet为模板
      
作者对ImageNet进行了跨范畴的定量分析，以评估道德违规的程度和基于模型注释的方法的可行性。这导致了ImageNet普查，需要对57个不同指标进行图像级和类级分析，这些指标包括计数、年龄和性别（CAG）、NSFW评分、类别标签的语义和使用预先训练的模型分类的准确性。
 
3


结论与讨论
作者试图引起机器学习界对大规模数据集的社会和伦理影响的关注，例如非一致同意的图像问题和经常隐藏的分类问题一直被认为是计算机视觉和人工智能领域最令人难以置信的突破之一。
ImageNet的成就确实值得庆祝，并且创造者们为解决一些伦理问题所做的努力也值得认可。尽管如此，ImageNet以及其他大型图像数据集仍然很麻烦。持续的沉默只会在将来造成更多的伤害而不是带来好处。在这方面，作者概述了一些解决办法，包括审计卡，可以考虑改善提出的一些关切。作者还策划了元数据集，并将代码开源，以ILSVRC2012数据集为模板进行定量审计。
作者敦促机器学习界密切关注他们的工作对社会，特别是对弱势群体的直接和间接影响。在这方面，必须意识到当前工作的历史前因、背景和政治层面。作者希望这项工作有助于提高人们的意识，并为继续讨论机器学习中的伦理和正义提供帮助。
 
4


一些其他观点
1、副本无处不在
即便MIT主动下线了Tiny Images数据集，但是数据副本无处不在。很多用户都下载过这些副本到本地，如何保证这些副本不会被再次上传到网络呢？在reddit上有网友表示知道该数据集的副本地址。
2、人工智能鉴黄系统的工作还能继续吗？
如果想要训练一个人工智能鉴黄系统，那么必须要先人为的制作数据集也就是要对一些图片打上标签说这是色情图片。
问题是这些图片从何而来呢？
如果是用爬虫程序从色情网站上收集，那么怎么保证这些图片当中哪些能用呢？比方说有些无辜受害的情侣被偷拍的照片被不法分子上传到色情网站，然后爬虫程序又把它们下载下来，我们难道可以哪怕是为了开发鉴黄系统而理所当然的使用这样照片吗？这难道不是对无辜受害者的隐私再一次侵犯吗？
另外如果说收集的是色情从业者（他们的国家合法化这项职业）的视频和图片，那TA们的肖像权就不值得尊重和保护了？
所以说一旦考虑到要严格遵守隐私权和肖像权，人工智能鉴黄系统就难以为继。
3、利用人工智能程序自动判断种族、性别等歧视是个矛盾
因为如果我们要考虑制造一个AI系统来自动帮助我们判别某些图片是不是存在某种歧视，那么我们同样需要收集和利用这些有歧视的图片，可是在得不到本人允许的情况下我们又何以冠冕堂皇的利用这些图片来做成“典型"来告诉人工智能说：嗨AI，快看！这个就是XX歧视的图片，你可得“记住”哈！
那就让我们“愉快”地抛弃人工智障回到农耕（手工）时代吧！
可是，难道个人或者企业私自收集并利用这些包含隐私/歧视的数据就合法了吗？？？
所以，如何建立一个公开的征得当事人同意的令公众信服的数据集就成为了当前和未来的一大难点。
参考内容：
\url{https://www.reddit.com/r/MachineLearning/comments/hjelz4/n_mit_permanently_pulls_offline_tiny_images/}
\url{https://arxiv.org/abs/2006.16923}
\url{https://www.theregister.com/2020/07/01/mit_dataset_removed/}1


